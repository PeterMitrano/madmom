import argparse

from datetime import datetime
import tensorflow as tf
import numpy as np


class BiDirectionalRNN:

    def __init__(self):
        num_features = 314
        self.forwards_x = tf.placeholder(tf.float32, shape=(None, None, num_features), name='forwards_x')
        self.labels = tf.placeholder(tf.float32, shape=(None, None, 2), name='labels')

        self.fwd_imgs = tf.transpose(tf.expand_dims(self.forwards_x, axis=3), perm=[0, 2, 1, 3])
        tf.summary.image("forwards input", self.fwd_imgs, max_outputs=10)

        self.backwards_x = tf.reverse(self.forwards_x, axis=[1], name='backwards_x')

        self.x = tf.concat((self.forwards_x, self.backwards_x), axis=2, name='concat')

        with tf.name_scope("lstm1"):
            self.w1 = tf.Variable(tf.truncated_normal([1, num_features*2, 25]), dtype=tf.float32, name='w1')
            self.b1 = tf.Variable(tf.truncated_normal([25]), dtype=tf.float32, name='b1')
            self.lstm1 = tf.tanh(tf.matmul(self.x, self.w1, name='matmul1') + self.b1)
            self.fw_lstm = tf.nn.rnn_cell.BasicRNNCell(num_features, activation=tf.tanh)
            self.bw_lstm = tf.nn.rnn_cell.BasicRNNCell(num_features, activation=tf.tanh)

        with tf.name_scope("fc"):
            self.fc_w1 = tf.Variable(tf.truncated_normal([1, 25, 2]))

        with tf.name_scope("softmax"):
            self.y_hat = tf.nn.softmax(self.lstm1, name='softmax')

        self.global_step = tf.Variable(0, trainable=False, name="global_step")

        with tf.name_scope('cross_entropy'):
            self.loss_raw = tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=self.lstm1, name='x_entropy')
            self.loss_sum = tf.reduce_sum(self.loss_raw, name='loss_reduce_sum')
            self.loss = tf.reduce_mean(self.loss_raw, name='loss_reduce_mean')
            tf.summary.scalar('loss', self.loss)

        with tf.name_scope('train'):
            self.train_step = tf.train.AdamOptimizer(0.001).minimize(self.loss, global_step=self.global_step)
            trainable_vars = tf.trainable_variables()
            grads = list(zip(tf.gradients(self.loss, trainable_vars), trainable_vars))
            for grad, var in grads:
                tf.summary.histogram(var.name + '/gradient', grad)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("dataset", help='dataset (npz file)')

    args = parser.parse_args()

    dataset = np.load(args.dataset)
    x = dataset['x']
    labels = dataset['labels']

    m = BiDirectionalRNN()

    summaries = tf.summary.merge_all()
    init_op = tf.global_variables_initializer()
    saver = tf.train.Saver()
    sess = tf.Session()
    sess.run(init_op)
    stamp = "{:%B_%d_%H:%M:%S}".format(datetime.now())
    writer = tf.summary.FileWriter('log_data/' + stamp)
    writer.add_graph(sess.graph)

    try:
        for i in range(x.shape[0]):
            batch_x = np.expand_dims(x[i], axis=0)
            batch_labels = np.concatenate((labels[i, ..., np.newaxis], 1 - labels[i, ..., np.newaxis]), axis=1)
            batch_labels = np.expand_dims(batch_labels, axis=0)
            s, = sess.run([summaries, m.train_step], feed_dict={m.forwards_x: batch_x, m.labels: batch_labels})
            writer.add_summary(s)
            print(i)

    except KeyboardInterrupt:
        pass

    saver.save(sess, 'final_project_model.ckpt')


if __name__ == '__main__':
    main()